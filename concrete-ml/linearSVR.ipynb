{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1087af6d",
   "metadata": {},
   "source": [
    "# Linear Regression using LinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f118fd",
   "metadata": {},
   "source": [
    "In this notebook, we show how to create, train and evaluate a linearSVR regression model using Concrete-ML library, our open-source privacy-preserving machine learning framework based on fully homomorphic encryption (FHE).\n",
    "\n",
    "For the sake of simplicity, we will only consider a single explanatory variable, making it easy to plot its relationship with the target variable.  \n",
    "\n",
    "In order to identify the best set of hyperparameters for the linearSVR, we perform a grid search on the following : \n",
    "\n",
    "* $C$: (inverse) strength of the l2 penalization\n",
    "* $\\epsilon$: margin for the support vectors\n",
    "\n",
    "Please refer to Scikit-Learn documentation on [linearSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d272c",
   "metadata": {},
   "source": [
    "## Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea25662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.svm import LinearSVR as SklearnLinearSVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "from concrete.ml.sklearn.svm import LinearSVR as ConcreteLinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a5450",
   "metadata": {},
   "source": [
    "And some helpers for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "train_plot_config = {\"c\": \"black\", \"marker\": \"D\", \"s\": 15, \"label\": \"Train data\"}\n",
    "test_plot_config = {\"c\": \"red\", \"marker\": \"x\", \"s\": 15, \"label\": \"Test data\"}\n",
    "\n",
    "def get_sklearn_plot_config(mse_score=None):\n",
    "    label = \"scikit-learn\"\n",
    "    if mse_score is not None:\n",
    "        label += f\", {'$MSE$'}={mse_score:.4f}\"\n",
    "    return {\"c\": \"blue\", \"linewidth\": 2.5, \"label\": label}\n",
    "\n",
    "def get_concrete_plot_config(mse_score=None):\n",
    "    label = \"Concrete-ML\"\n",
    "    if mse_score is not None:\n",
    "        label += f\", {'$MSE$'}={mse_score:.4f}\"\n",
    "    return {\"c\": \"orange\", \"linewidth\": 2.5, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e092d",
   "metadata": {},
   "source": [
    "## Generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2dc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "# Use only one feature for educational purpose\n",
    "X = X[:, np.newaxis, 2]\n",
    "\n",
    "# We split the dataset into a training and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=23)\n",
    "\n",
    "# We sort the test set for a better visualization\n",
    "sorted_indexes = np.argsort(np.squeeze(X_test))\n",
    "X_test = X_test[sorted_indexes, :]\n",
    "y_test = y_test[sorted_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0975f5",
   "metadata": {},
   "source": [
    "We display the dataset to visualize the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, figsize=(10, 5))\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "ax.scatter(X_train, y_train, **train_plot_config)\n",
    "ax.scatter(X_test, y_test, **test_plot_config)\n",
    "ax.legend()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931bce92",
   "metadata": {},
   "source": [
    "## Identify best set of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf941b",
   "metadata": {},
   "source": [
    "###  Sklearn LinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860fd50",
   "metadata": {},
   "source": [
    "Create scorer with the [Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_scorer = make_scorer(mean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f864d02e",
   "metadata": {},
   "source": [
    "We train the scikit-learn LinearSVR model on clear data.\n",
    "\n",
    "We use a parameter grid with several values for $\\epsilon$ and $C$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"epsilon\": [0.0, 1.0, 10.0, 20.0],\n",
    "    \"C\": [0.1, 100.0, 10000.0, 100000.0],\n",
    "}\n",
    "\n",
    "sklearn_rgs = SklearnLinearSVR()\n",
    "kfold_cv = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "\n",
    "gs_sklearn = GridSearchCV(\n",
    "    sklearn_rgs,\n",
    "    param_grid,\n",
    "    cv=kfold_cv,\n",
    "    scoring=grid_scorer,\n",
    "    verbose=1,\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e7c24",
   "metadata": {},
   "source": [
    "###  Concrete-ML quantized linearSVR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f82a8c7d",
   "metadata": {},
   "source": [
    "The typical development flow of a Concrete-ML model is the following:\n",
    "\n",
    "* The model is trained on clear (plaintext) data as only FHE inference is currently supported\n",
    "* The resulting trained model is quantized using a `n_bits` parameter set by the user (see documentation [here](https://docs.zama.ai/concrete-ml/developer-guide/api/concrete.ml.sklearn.svm#class-linearsvr)). This parameter can either be:\n",
    "    1. a dictionary composed of `op_inputs` and `op_weights` keys. These parameters are given as integers representing the number of bits over which the associated data should be quantized.\n",
    "    2. an integer, representing the number of bits over which each input and weight should be quantized. Default is 8. We try several values to test the gain of various precisions for quantization. \n",
    "* The quantized model is compiled to a FHE equivalent following 3 steps:\n",
    "    1. create an executable operation graph\n",
    "    2. check that the op-graph is FHE compatible by checking the maximum bit-width needed to execute the model\n",
    "    3. determine cryptographic parameters that will help to generate the secret keys and evaluation keys. If no parameters can be found, the compilation process can't be completed and an error is thrown. The user then can either lower the value(s) chosen for `n_bits` or decrease the number of features found in the data set (using techniques such as [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)) and run the development flow once again.\n",
    "* Inference can then be done on encrypted data (using `fhe=\"execute\"` when calling `.predict()` method. See bellow)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f6c5f40",
   "metadata": {},
   "source": [
    "We use the same grid of parameter values. We test several values for `n_bits` : `[6, 8, 12]` (see explanation in following sections). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6126ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_bits\": [6, 8, 12],\n",
    "    \"epsilon\": [0.0, 1.0, 10.0, 20.0],\n",
    "    \"C\": [0.1, 100.0, 10000.0, 100000.0],\n",
    "}\n",
    "\n",
    "concrete_rgs = ConcreteLinearSVR()\n",
    "\n",
    "gs_concrete = GridSearchCV(\n",
    "    concrete_rgs,\n",
    "    param_grid,\n",
    "    cv=kfold_cv,\n",
    "    scoring=grid_scorer,\n",
    "    verbose=1,\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac24379",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "results_df = pd.DataFrame(gs_concrete.cv_results_)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "l1, = ax.plot(np.arange(16), -results_df.loc[results_df[\"param_n_bits\"]==6,\"mean_test_score\"], '-o')\n",
    "l2, = ax.plot(np.arange(16), -results_df.loc[results_df[\"param_n_bits\"]==8,\"mean_test_score\"], '-o')\n",
    "l3, = ax.plot(np.arange(16), -results_df.loc[results_df[\"param_n_bits\"]==12,\"mean_test_score\"], '-o')\n",
    "ax.legend((l1, l2, l3), ('n_bits = 6', 'n_bits = 8', 'n_bits = 12'), loc='upper right', shadow=True)\n",
    "ax.set_xlabel('Different models with fixed values of C and epsilon')\n",
    "ax.set_ylabel('Mean MSE accros CV folds')\n",
    "ax.set_title('Impact of `n_bits` on Cross Validation performances')\n",
    "display(fig)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9765fd9c",
   "metadata": {},
   "source": [
    "As can be seen on the graph, on this fairly simple dataset with only a single feature and few points, and with a fairly simple model with only few parameters for the decision rule, the complexity of the information to be represented as integers is not huge. This results here, on `n_bits` values having not much of an impact on the perfomance of the model. \n",
    "\n",
    "One can observe here that for the best model, performances with `n_bits` equals to `6`, `8` or `12` are quite close. Performances differ a bit much for models with `C = 100000.0`, meaning the `l2` penalty is weak and the decision rule can adjust more easily to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991386ee",
   "metadata": {},
   "source": [
    "### Compare Sklearn and Concrete-ML Quantized best models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f94e0",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e884473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print mean time fit and std time fit for both models\n",
    "print(\n",
    "    f\"Mean time fit sklearn: {np.mean(gs_sklearn.cv_results_['mean_fit_time']):.3f}s,\"\n",
    "    f\" std time fit sklearn: {np.std(gs_sklearn.cv_results_['mean_fit_time']):.3f}s\"\n",
    ")\n",
    "print(\n",
    "    f\"Mean time fit concrete: {np.mean(gs_concrete.cv_results_['mean_fit_time']):.3f}s,\"\n",
    "    f\"std time fit concrete: {np.std(gs_concrete.cv_results_['mean_fit_time']):.3f}s\"\n",
    ")\n",
    "\n",
    "# Print best score for both models\n",
    "print(f\"Best MSE score sklearn: {-gs_sklearn.best_score_:.2f}\")\n",
    "print(f\"Best MSE score concrete: {-gs_concrete.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0143f",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5027bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyper parameters out of gs_concrete\n",
    "best_params_concrete = gs_concrete.best_params_\n",
    "print(f\"Best parameters for Concrete: {best_params_concrete}\")\n",
    "best_params_sklearn = gs_sklearn.best_params_\n",
    "print(f\"Best parameters for Sklearn: {best_params_sklearn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ef351",
   "metadata": {},
   "source": [
    "### Train with best hyperparameter set on the complete training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d61559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train concrete and sklearn LinearSVR  with best hyper parameters\n",
    "concrete_rgs = ConcreteLinearSVR(**best_params_concrete)\n",
    "sklearn_rgs = SklearnLinearSVR(**best_params_sklearn)\n",
    "\n",
    "concrete_rgs.fit(X_train, y_train)\n",
    "sklearn_rgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9251a9d",
   "metadata": {},
   "source": [
    "## Concrete-ML Quantized linearSVR with FHE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcbfc0",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eba8bb8",
   "metadata": {},
   "source": [
    "Some prerequisites should be reviewed before deep diving !\n",
    "\n",
    "Quantization is a technique that converts continuous data (floating point, e.g., in 32-bits) to discrete numbers within a fixed range (in our case either 6, 8 or 12 bits). This means that some information is lost during the process. However, the larger is the integers' range, the smaller the error becomes, making it acceptable to some cases.\n",
    "\n",
    "To learn more about quantization, please refer to this [page](https://docs.zama.ai/concrete-ml/advanced-topics/quantization).\n",
    "\n",
    "Regarding FHE, the input data type must be represented exclusively as integers, making the use of quantization necessary. Therefore, a linear model trained on floats is quantized into an equivalent integer model using *Post-Training Quantization*. This operation can lead to a loss of accuracy compared to the standard floating point models working on clear data.\n",
    "\n",
    "In practice however, this loss is usually very limited with linear FHE models as they can consider very large integers, with up to 50 bits in some cases. This means these models can quantize their inputs and weights over large number of bits while still considering data sets containing many features (e.g. 1000). We therefore often observe almost identical performance scores between float, quantized and FHE models.\n",
    "\n",
    "To learn more about the relation between the maximum bit-width reached within a model, the bits of quantization used and the dataset's number of features, please refer to this [page](https://docs.zama.ai/concrete-ml/advanced-topics/pruning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e835f8",
   "metadata": {},
   "source": [
    "### Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0e874",
   "metadata": {},
   "source": [
    "To perform homomorphic inference, we take the above trained quantized model and we compile it to get a FHE model.\n",
    "\n",
    "The compiler requires an exhaustive set of data to evaluate the maximum integer bit-width within the graph, which is needed during the FHE computations before running any predictions.\n",
    "\n",
    "The user can either provide the entire train dataset or a smaller but representative subset of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9590403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using the training data\n",
    "circuit = concrete_rgs.compile(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33827f6",
   "metadata": {},
   "source": [
    "### Generate the key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a9172",
   "metadata": {},
   "source": [
    "The compiler returns a circuit, which can then be used for key generation and predictions. More precisely, it generates:\n",
    "\n",
    "* a Secret Key, used for the encryption and decryption processes. This key should remain accessible only to the user.\n",
    "* an Evaluation Key, used to evaluate the circuit on encrypted data. Anyone could access this key without breaching the model's security.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the key\n",
    "print(f\"Generating a key for an {circuit.graph.maximum_integer_bit_width()}-bit circuit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_begin = time.time()\n",
    "circuit.client.keygen(force=False)\n",
    "print(f\"Key generation time: {time.time() - time_begin:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752452bc",
   "metadata": {},
   "source": [
    "### Now let's predict using the FHE model on encrypted data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b71e63e",
   "metadata": {},
   "source": [
    "Please, notice the `fhe=\"execute\"` that makes the job under the hood: before the data is sent to be applied to the model, it is encrypted with the client secret key generated above. \n",
    "\n",
    "As for future comparison bellow, we also predict on the very same data on both:\n",
    "* the Sklearn model,\n",
    "* the Concrete-ML quantized model without FHE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict using the FHE-quantized model on the testing set\n",
    "time_begin = time.time()\n",
    "y_pred_fhe = concrete_rgs.predict(X_test, fhe=\"execute\")\n",
    "print(f\"Execution time: {(time.time() - time_begin) / len(X_test):.4f} seconds per sample\")\n",
    "\n",
    "# Now predict using the Sklearn model on the testing set\n",
    "time_begin = time.time()\n",
    "y_pred_sklearn = sklearn_rgs.predict(X_test)\n",
    "print(f\"Execution time: {(time.time() - time_begin) / len(X_test):.4f} seconds per sample\")\n",
    "\n",
    "# Now predict using clear quantized Concrete-ML model on testing set\n",
    "time_begin = time.time()\n",
    "y_preds_quantized = concrete_rgs.predict(X_test)\n",
    "print(f\"Execution time: {(time.time() - time_begin) / len(X_test):.4f} seconds per sample\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e99e85",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8c76d",
   "metadata": {},
   "source": [
    "### Display performance spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all MSE a string to explain\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "mse_clear = mean_squared_error(y_test, y_preds_quantized)\n",
    "mse_fhe = mean_squared_error(y_test, y_pred_fhe)\n",
    "\n",
    "print(\n",
    "    f\"Clear FP32 sklearn model MSE: {mse_sklearn:.3f}\\n\"\n",
    "    f\"Clear quantized model MSE: {mse_clear:.3f}\\n\"\n",
    "    f\"FHE model MSE: {mse_fhe:.3f}\"\n",
    ")\n",
    "\n",
    "# Measure the error of the FHE quantized model with respect to quantized clear Concrete-ML model\n",
    "concrete_score_difference = abs(mse_fhe - mse_clear) * 100 / mse_clear\n",
    "print(\n",
    "    \"\\nRelative difference between Concrete-ml (quantized clear) and Concrete-ml (FHE) scores:\",\n",
    "    f\"{concrete_score_difference:.2f}%\",\n",
    ")\n",
    "\n",
    "\n",
    "# Measure the error of the FHE quantized model with respect to the sklearn float model\n",
    "score_difference = abs(mse_fhe - mse_sklearn) * 100 / mse_sklearn\n",
    "print(\n",
    "    \"Relative difference between scikit-learn (clear) and Concrete-ml (FHE) scores:\",\n",
    "    f\"{score_difference:.2f}%\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18786764",
   "metadata": {},
   "source": [
    "We can observe that scikit-learn and Concrete-ML (quantized clear) models output very close scores. This demonstrate how the quantization process has a very limited impact on performances.\n",
    "\n",
    "We can observe as well that the performance difference between Concrete-ML (quantized clear) and Concrete-ML (FHE) is quasi null. This demonstrate that the compilation process has a very limited impact on performance. If one observed a more significant difference, then `n_bits` can be increased to offer more degrees of freedom during the compilation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87023b7e",
   "metadata": {},
   "source": [
    "### Visualize decision rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ae5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We densify the space representation of the original X,\n",
    "# to better visualize the resulting step function in the following figure\n",
    "x_space = np.linspace(X_test.min(), X_test.max(), num=300)\n",
    "x_space = x_space[:, np.newaxis]\n",
    "y_pred_q_space = concrete_rgs.predict(x_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1825a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "ax.scatter(X_train, y_train, **train_plot_config)\n",
    "ax.scatter(X_test, y_test, **test_plot_config)\n",
    "ax.plot(X_test, y_pred_sklearn, **get_sklearn_plot_config(mse_sklearn))\n",
    "ax.plot(x_space, y_pred_q_space, **get_concrete_plot_config(mse_clear))\n",
    "ax.legend()\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1461c97b",
   "metadata": {},
   "source": [
    "On the above graph, one can see that test dataset has a point for which `X` value is outside the range of train dataset. Since, when we compiled the quantized model we used `X_train`, this `X` values in the test dataset was not seen by the compilation process, then the decision rule poorly generalizes on those values outside the boundaries observed on `X_tain`. An alternative to correct this would be to give at model compilation time, a wider range of values for `X`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141bc829",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c44fff29",
   "metadata": {},
   "source": [
    "In this tutorial, we have shown how easy it is to train and execute a linearSVR regression model in FHE using Concrete-ML.\n",
    "\n",
    "We have also discussed the development flow of a FHE model: training, quantization, compilation and inference.\n",
    "\n",
    "Prediction performances are quasi identical. The tiny difference is explained by the quantization process and the compilation of the trained model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
